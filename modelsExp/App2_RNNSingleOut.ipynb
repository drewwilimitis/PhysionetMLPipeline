{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7983bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec59e54",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9616a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "### SIMPLE BBCE ###\n",
    "\n",
    "def create_simple_temp_weight(y, hyperparameters, timeSteps=14):\n",
    "    \"\"\"Create simple temporal weights for binary cross-entropy based on class imbalance.\n",
    "    \n",
    "    Args:\n",
    "        y (ndarray): Array of binary labels with shape (P, T).\n",
    "        hyperparameters (dict): Dictionary containing hyperparameters.\n",
    "        timeSteps (int): Number of time steps (optional, for future use).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Sample weights with the same shape as y.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input array to float32\n",
    "    sample_weights = y.copy().astype(np.float32)\n",
    "    \n",
    "    # Count the number of positive and negative samples\n",
    "    num_positive = np.sum(y == 1)\n",
    "    num_negative = np.sum(y == 0)\n",
    "    \n",
    "    # Calculate the total number of samples\n",
    "    total_samples = num_positive + num_negative\n",
    "    \n",
    "    # Calculate the proportion of each class\n",
    "    positive_proportion = num_positive / total_samples\n",
    "    negative_proportion = num_negative / total_samples\n",
    "    \n",
    "    # Calculate the weights inversely proportional to the class proportions\n",
    "    positive_weight = 1.0 / positive_proportion\n",
    "    negative_weight = 1.0 / negative_proportion\n",
    "    \n",
    "    # Assign weights: calculated weight for positive samples and calculated weight for negative samples\n",
    "    sample_weights[np.where(sample_weights == 1)] = positive_weight\n",
    "    sample_weights[np.where(sample_weights == 0)] = negative_weight\n",
    "    \n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "def reset_keras(seed=42):\n",
    "    \"\"\"Function to ensure that results from Keras models\n",
    "    are consistent and reproducible across different runs\"\"\"\n",
    "    \n",
    "    K.clear_session()\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    \"\"\"\n",
    "    GRUModel class builds and trains a Gated Recurrent Unit (GRU) model\n",
    "    with specified layers and hyperparameters.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    hyperparameters : dict\n",
    "        A dictionary containing key hyperparameters for model building and training.\n",
    "        \n",
    "    Methods:\n",
    "    --------\n",
    "    build_model(lr_sch):\n",
    "        Builds the GRU model with the specified learning rate scheduler.\n",
    "    train(x_train, y_train, epochs, batch_size, validation_data):\n",
    "        Trains the built model with the provided training and validation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hyperparameters):\n",
    "        \"\"\"\n",
    "        Initializes the GRUModel with hyperparameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hyperparameters : dict\n",
    "            A dictionary containing key hyperparameters for model building and training.\n",
    "        \"\"\"\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    def build_model(self, lr_sch):\n",
    "        \"\"\"\n",
    "        Builds the GRU model with specified learning rate scheduler.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lr_sch : float\n",
    "            Learning rate for the optimizer during training.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model : tf.keras.Model\n",
    "            The compiled GRU model.\n",
    "        \"\"\"\n",
    "        # Define input layer with dynamic shape and masking\n",
    "        dynamic_input = tf.keras.layers.Input(shape=(self.hyperparameters[\"timeStep\"], self.hyperparameters[\"layers\"][0]))\n",
    "        masked = tf.keras.layers.Masking(mask_value=self.hyperparameters['maskValue'])(dynamic_input)\n",
    "        \n",
    "        # Define GRU layer with specified parameters\n",
    "        gru_encoder = tf.keras.layers.GRU(\n",
    "            self.hyperparameters['layers'][1],\n",
    "            dropout=self.hyperparameters['dropout'],\n",
    "            return_sequences=False,\n",
    "            activation='tanh',\n",
    "            use_bias=True\n",
    "        )(masked)\n",
    "\n",
    "        # Define output layer with sigmoid activation function\n",
    "        output = tf.keras.layers.Dense(1, use_bias=False, activation=\"sigmoid\")(gru_encoder)\n",
    "        \n",
    "        # Compile the model with Adam optimizer and custom loss function\n",
    "        model = tf.keras.Model(dynamic_input, [output])\n",
    "        my_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_sch)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=my_optimizer,\n",
    "                      metrics=['accuracy', 'AUC']\n",
    "                     )\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val, w1, w2):\n",
    "        \"\"\"\n",
    "        Trains the built model with provided training and validation data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_train : numpy array\n",
    "            Input training data.\n",
    "        y_train : numpy array\n",
    "            Target training data.\n",
    "        epochs : int\n",
    "            Number of training epochs.\n",
    "        batch_size : int\n",
    "            Batch size for training.\n",
    "        validation_data : tuple\n",
    "            Tuple containing input and target validation data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        history : tf.keras.callbacks.History\n",
    "            A record of training loss values and metrics values at successive epochs.\n",
    "        model : tf.keras.Model\n",
    "            The trained GRU model.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.build_model(lr_sch=self.hyperparameters['lr_scheduler'])\n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      min_delta=self.hyperparameters[\"mindelta\"],\n",
    "                                                      patience=self.hyperparameters[\"patience\"],\n",
    "                                                      restore_best_weights=True,\n",
    "                                                      mode=\"min\")\n",
    "                    \n",
    "        history = model.fit(x_train, y_train,\n",
    "                            validation_data=(x_val, y_val, w2.squeeze()),\n",
    "                            callbacks=[earlystopping],\n",
    "                            batch_size=self.hyperparameters['batch_size'], \n",
    "                            epochs=self.hyperparameters['epochs'],\n",
    "                            verbose=0,\n",
    "                            sample_weight = w1.squeeze())\n",
    "\n",
    "        return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6983496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def evaluate_combination(hyperparameters, seed, X_train, y_train, k, l, m, dropout, layers, lr_scheduler):\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "    hyperparameters_copy['dropout'] = dropout[k]\n",
    "    hyperparameters_copy['layers'] = layers[l]\n",
    "    hyperparameters_copy['lr_scheduler'] = lr_scheduler[m]\n",
    "    \n",
    "    v_val_loss = []\n",
    "    v_hist = []\n",
    "\n",
    "#     print(\"\\t\\tLearning rate:\", lr_scheduler[m], \", dropout:\", dropout[k], \", layers:\", layers[l])\n",
    "    \n",
    "    all_patients_train = X_train.shape[0]\n",
    "    kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=seed)\n",
    "    kf.get_n_splits(all_patients_train)\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_cv = X_train[train_index]\n",
    "        X_val_cv = X_train[val_index]\n",
    "        y_train_cv = y_train[train_index]\n",
    "        y_val_cv = y_train[val_index]\n",
    "\n",
    "        sample_weights_train = create_simple_temp_weight(y_train_cv, hyperparameters_copy, timeSteps=14)\n",
    "        sample_weights_val = create_simple_temp_weight(y_val_cv, hyperparameters_copy, timeSteps=14)\n",
    "\n",
    "        reset_keras()\n",
    "        model = GRUModel(hyperparameters_copy)\n",
    "        hist, model = model.train(X_train_cv, y_train_cv, X_val_cv, y_val_cv, sample_weights_train, sample_weights_val)\n",
    "\n",
    "        v_hist.append(hist)\n",
    "        v_val_loss.append(np.max(hist.history[\"val_AUC\"]))\n",
    "\n",
    "    metric_dev = np.mean(v_val_loss)\n",
    "    return (metric_dev, k, l, m, X_train_cv, y_train_cv, X_val_cv, y_val_cv, v_hist)\n",
    "\n",
    "def myCVGridParallel(hyperparameters, seed, X_train, y_train):\n",
    "    bestHyperparameters = {'dropout': -1, 'layers': -1, 'lr_scheduler': -1}\n",
    "    bestMetricDev = -np.inf\n",
    "    \n",
    "    lr_scheduler = hyperparameters[\"lr_scheduler\"]\n",
    "    layers = hyperparameters[\"layers\"]\n",
    "    dropout = hyperparameters[\"dropout\"]\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(evaluate_combination)(hyperparameters, seed, X_train, y_train, k, l, m, dropout, layers, lr_scheduler)\n",
    "        for k in range(len(dropout))\n",
    "        for l in range(len(layers))\n",
    "        for m in range(len(lr_scheduler))\n",
    "    )\n",
    "\n",
    "    for metric_dev, k, l, m, X_train_cv, y_train_cv, X_val_cv, y_val_cv, v_hist in results:\n",
    "        if metric_dev > bestMetricDev:\n",
    "#             print(\"\\t\\t\\tCambio the best\", bestMetricDev, \"por metric dev:\", metric_dev)\n",
    "            bestMetricDev = metric_dev\n",
    "            bestHyperparameters['dropout'] = k\n",
    "            bestHyperparameters['layers'] = l\n",
    "            bestHyperparameters['lr_scheduler'] = m\n",
    "            bestHyperparameters['X_train'] = X_train_cv\n",
    "            bestHyperparameters['y_train'] = y_train_cv\n",
    "            bestHyperparameters['X_val'] = X_val_cv\n",
    "            bestHyperparameters['y_val'] = y_val_cv\n",
    "\n",
    "    return bestHyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f27ecf",
   "metadata": {},
   "source": [
    "## Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0412d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "folders = [\"s1\"]\n",
    "seeds = [143, 45, 67]\n",
    "i = 0\n",
    "idx_exp = 2\n",
    "X_train = np.load(\"../Data/splits/App\" +str(idx_exp)+ \"/\"  + folders[i] + \"/X_train_tensor.npy\")\n",
    "\n",
    "input_shape = X_train.shape[2]\n",
    "# Select the first 24h - 24 time steps\n",
    "timeStep = 6\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "layer_list = [\n",
    "    [input_shape, 3, 1],\n",
    "    [input_shape, 5, 1],\n",
    "    [input_shape, 8, 1],\n",
    "    [input_shape, 12, 1],\n",
    "    [input_shape, 15, 1],\n",
    "]\n",
    "dropout = [0, 0.15, 0.3]\n",
    "lr_scheduler = [1e-1, 1e-2, 1e-3]\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"timeStep\": timeStep,\n",
    "    \"maskValue\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": 30,\n",
    "    \"kfold\": 5,\n",
    "    \"dropout\": dropout,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"layers\": layer_list,\n",
    "    \"verbose\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd2713e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "/tmp/ipykernel_1244310/555271259.py:32: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 6, 15), found shape=(None, 7, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/splits/App\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx_exp)\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;241m+\u001b[39m folders[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/y_test_tensor.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#GridSearch of hyperparameters and print them   \u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m bestHyperparameters \u001b[38;5;241m=\u001b[39m myCVGridParallel(hyperparameters, seeds[i], X_train, y_train)\n\u001b[1;32m     21\u001b[0m bestHyperparameters_bySplit[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m bestHyperparameters\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlr_sch seleccionado:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr_scheduler[bestHyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36mmyCVGridParallel\u001b[0;34m(hyperparameters, seed, X_train, y_train)\u001b[0m\n\u001b[1;32m     45\u001b[0m     dropout \u001b[38;5;241m=\u001b[39m hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     47\u001b[0m     num_cores \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[0;32m---> 48\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_cores)(\n\u001b[1;32m     49\u001b[0m         delayed(evaluate_combination)(hyperparameters, seed, X_train, y_train, k, l, m, dropout, layers, lr_scheduler)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dropout))\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers))\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lr_scheduler))\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric_dev, k, l, m, X_train_cv, y_train_cv, X_val_cv, y_val_cv, v_hist \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metric_dev \u001b[38;5;241m>\u001b[39m bestMetricDev:\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#             print(\"\\t\\t\\tCambio the best\", bestMetricDev, \"por metric dev:\", metric_dev)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 6, 15), found shape=(None, 7, 15)"
     ]
    }
   ],
   "source": [
    "metrics_data = []\n",
    "\n",
    "loss_train = []\n",
    "loss_dev = []\n",
    "v_models = []\n",
    "bestHyperparameters_bySplit = {}\n",
    "y_pred_by_split = []\n",
    "\n",
    "\n",
    "for i in range(0,3):\n",
    "\n",
    "    X_train = np.load(\"../Data/splits/App\" +str(idx_exp)+ \"/\"  + folders[i] + \"/X_train_tensor.npy\")\n",
    "    y_train = np.load(\"../Data/splits/App\" +str(idx_exp)+ \"/\"  + folders[i] + \"/y_train_tensor.npy\")\n",
    "    \n",
    "    X_test = np.load(\"../Data/splits/App\" +str(idx_exp)+ \"/\"  + folders[i] + \"/X_test_tensor.npy\")\n",
    "    y_test = np.load(\"../Data/splits/App\" +str(idx_exp)+ \"/\"  + folders[i] + \"/y_test_tensor.npy\")\n",
    "\n",
    "    #GridSearch of hyperparameters and print them   \n",
    "    bestHyperparameters = myCVGridParallel(hyperparameters, seeds[i], X_train, y_train)\n",
    "    \n",
    "    bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "    print(\"\\tlr_sch seleccionado:\", lr_scheduler[bestHyperparameters[\"lr_scheduler\"]])\n",
    "    print(\"\\tdropout seleccionado:\", dropout[bestHyperparameters[\"dropout\"]])\n",
    "    print(\"\\tlayers seleccionado:\", layer_list[bestHyperparameters[\"layers\"]])\n",
    "    \n",
    "    \n",
    "    besthyperparameters = {\n",
    "        'timeStep': hyperparameters[\"timeStep\"],\n",
    "        'maskValue': hyperparameters[\"maskValue\"],\n",
    "        'batch_size': hyperparameters[\"batch_size\"],\n",
    "        'epochs': hyperparameters[\"epochs\"],\n",
    "        'monitor':  hyperparameters[\"monitor\"],\n",
    "        \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "        \"patience\": hyperparameters[\"patience\"],                    \n",
    "        \"dropout\": dropout[bestHyperparameters[\"dropout\"]],\n",
    "        \"layers\": layer_list[bestHyperparameters[\"layers\"]],\n",
    "        \"lr_scheduler\": lr_scheduler[bestHyperparameters[\"lr_scheduler\"]],                    \n",
    "        'kfold': hyperparameters[\"kfold\"],\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    X_train = bestHyperparameters[\"X_train\"]\n",
    "    y_train = bestHyperparameters[\"y_train\"]\n",
    "    X_val = bestHyperparameters[\"X_val\"]\n",
    "    y_val = bestHyperparameters[\"y_val\"]\n",
    "    \n",
    "#--- TRY ON TEST -----------------------------------------------------------------------#\n",
    "\n",
    "    #Reset keras\n",
    "    reset_keras()\n",
    "    model = GRUModel(besthyperparameters)\n",
    "    sample_weights_train = create_simple_temp_weight(y_train, hyperparameters, timeSteps=14)\n",
    "    sample_weights_val = create_simple_temp_weight(y_val, hyperparameters, timeSteps=14)\n",
    "\n",
    "    hist, model = model.train(X_train, y_train, X_val, y_val, sample_weights_train, sample_weights_val)\n",
    "\n",
    "    y_pred = model.predict(x=X_test)\n",
    "    y_pred = np.reshape(y_pred, (y_pred.size,))\n",
    "    y_pred_by_split.append(y_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0462eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_by_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de439c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    y_pred_final = y_pred_by_split[i]\n",
    "    y_test = np.load(\"../0_Data/splits/App\" +str(idx_exp)+ \"/\"  + folders[i] + \"/y_test_tensor.npy\")\n",
    "\n",
    "    #--- METRICS -----------------------------------------------------------------------#     \n",
    "    accuracy_test = sklearn.metrics.accuracy_score(y_test, np.round(y_pred_final))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, np.round(y_pred_final)).ravel()\n",
    "    roc = sklearn.metrics.roc_auc_score(y_test, y_pred_final)\n",
    "\n",
    "    accuracy = accuracy_test\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn) \n",
    "    f1score =  (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    metrics = {\n",
    "        \"S\": i,  \n",
    "        \"TN\": tn,\n",
    "        \"TP\": tp,\n",
    "        \"FN\": fn,\n",
    "        \"FP\": fp,\n",
    "        \"ACC\": accuracy,\n",
    "        \"SPEC\": specificity,\n",
    "        \"PREC\": precision,\n",
    "        \"RECALL\": recall,\n",
    "        \"F1\": f1score,\n",
    "        \"ROC\": roc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

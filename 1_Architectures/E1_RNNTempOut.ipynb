{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7983bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec59e54",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fe3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras(seed=42):\n",
    "    \"\"\"Function to ensure that results from Keras models\n",
    "    are consistent and reproducible across different runs\"\"\"\n",
    "    \n",
    "    K.clear_session()\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    \"\"\"\n",
    "    GRUModel class builds and trains a Gated Recurrent Unit (GRU) model\n",
    "    with specified layers and hyperparameters.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    hyperparameters : dict\n",
    "        A dictionary containing key hyperparameters for model building and training.\n",
    "        \n",
    "    Methods:\n",
    "    --------\n",
    "    build_model(lr_sch):\n",
    "        Builds the GRU model with the specified learning rate scheduler.\n",
    "    train(x_train, y_train, epochs, batch_size, validation_data):\n",
    "        Trains the built model with the provided training and validation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hyperparameters):\n",
    "        \"\"\"\n",
    "        Initializes the GRUModel with hyperparameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hyperparameters : dict\n",
    "            A dictionary containing key hyperparameters for model building and training.\n",
    "        \"\"\"\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    def build_model(self, lr_sch):\n",
    "        \"\"\"\n",
    "        Builds the GRU model with specified learning rate scheduler.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lr_sch : float\n",
    "            Learning rate for the optimizer during training.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model : tf.keras.Model\n",
    "            The compiled GRU model.\n",
    "        \"\"\"\n",
    "        # Define input layer with dynamic shape and masking\n",
    "        dynamic_input = tf.keras.layers.Input(shape=(self.hyperparameters[\"timeStep\"], self.hyperparameters[\"layers\"][0]))\n",
    "        masked = tf.keras.layers.Masking(mask_value=self.hyperparameters['maskValue'])(dynamic_input)\n",
    "        \n",
    "        # Define GRU layer with specified parameters\n",
    "        gru_encoder = tf.keras.layers.GRU(\n",
    "            self.hyperparameters['layers'][1],\n",
    "            dropout=self.hyperparameters['dropout'],\n",
    "            return_sequences=True,\n",
    "            activation='tanh',\n",
    "            use_bias=True\n",
    "        )(masked)\n",
    "\n",
    "        # Define output layer with sigmoid activation function\n",
    "        output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, use_bias=False, activation=\"sigmoid\"))(gru_encoder)\n",
    "        \n",
    "        # Compile the model with Adam optimizer and custom loss function\n",
    "        model = tf.keras.Model(dynamic_input, [output])\n",
    "        my_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_sch)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=my_optimizer,\n",
    "                      metrics=['accuracy', 'AUC']\n",
    "                     )\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"\n",
    "        Trains the built model with provided training and validation data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_train : numpy array\n",
    "            Input training data.\n",
    "        y_train : numpy array\n",
    "            Target training data.\n",
    "        epochs : int\n",
    "            Number of training epochs.\n",
    "        batch_size : int\n",
    "            Batch size for training.\n",
    "        validation_data : tuple\n",
    "            Tuple containing input and target validation data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        history : tf.keras.callbacks.History\n",
    "            A record of training loss values and metrics values at successive epochs.\n",
    "        model : tf.keras.Model\n",
    "            The trained GRU model.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.build_model(lr_sch=self.hyperparameters['lr_scheduler'])\n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      min_delta=self.hyperparameters[\"mindelta\"],\n",
    "                                                      patience=self.hyperparameters[\"patience\"],\n",
    "                                                      restore_best_weights=True,\n",
    "                                                      mode=\"min\")\n",
    "                    \n",
    "        history = model.fit(x_train, y_train,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            callbacks=[earlystopping],\n",
    "                            batch_size=self.hyperparameters['batch_size'], \n",
    "                            epochs=self.hyperparameters['epochs'],\n",
    "                            verbose=self.hyperparameters['verbose'])\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "def myCVGrid(hyperparameters, seed, X_train, y_train):\n",
    "    \"\"\"Grid Search. Calculate metricDev based on the evaluation. Compares the metricDev with the current bestMetricDev. \n",
    "       If better, updates bestMetricDev and stores those hyperparameters in bestHyperparameters.\n",
    "       \n",
    "       Returns:\n",
    "          - bestHyperparameters (dict)\n",
    "          - X_train, X_val, y_train, y_val (arrays): Training and validation datasets.\n",
    "          - v_early (list): Early stopping information for each hyperparameter combination.\n",
    "          - v_hist (list): Training history for each hyperparameter combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    bestHyperparameters = {'dropout': -1, 'layers': -1, 'lr_scheduler':-1}\n",
    "    bestMetricDev = np.inf\n",
    "    \n",
    "    \n",
    "    lr_scheduler = hyperparameters[\"lr_scheduler\"]\n",
    "    layers = hyperparameters[\"layers\"]\n",
    "    dropout = hyperparameters[\"dropout\"]\n",
    "    \n",
    "    for k in range(len(dropout)):\n",
    "        for l in range(len(layers)):\n",
    "            for m in range(len(lr_scheduler)):\n",
    "                hyperparameters = {\n",
    "                    'timeStep': hyperparameters[\"timeStep\"],\n",
    "                    'maskValue': hyperparameters[\"maskValue\"],\n",
    "                    'batch_size': hyperparameters[\"batch_size\"],\n",
    "                    'epochs': hyperparameters[\"epochs\"],\n",
    "                    'monitor':  hyperparameters[\"monitor\"],\n",
    "                    \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "                    \"patience\": hyperparameters[\"patience\"],\n",
    "                    \"kfold\": hyperparameters[\"kfold\"],\n",
    "                    \n",
    "                    \"dropout\": dropout[k],\n",
    "                    \"layers\": layers[l],\n",
    "                    \"lr_scheduler\": lr_scheduler[m],\n",
    "                \n",
    "                    'verbose': 0\n",
    "                }\n",
    "                v_metric_dev = []\n",
    "                v_hist = []\n",
    "                v_val_loss = []\n",
    "                print(\"\\t\\tLearning rate:\", lr_scheduler[m], \", dropout:\", dropout[k], \", layers:\", layers[l])\n",
    "\n",
    "               \n",
    "                all_patients_train = X_train.shape[0]\n",
    "                kf = KFold(n_splits=hyperparameters[\"kfold\"], shuffle=True, random_state=seed)\n",
    "                kf.get_n_splits(all_patients_train)\n",
    "                for train_index, val_index in kf.split(X_train):\n",
    "                    X_train_cv = X_train[train_index]\n",
    "                    X_val_cv = X_train[val_index]\n",
    "                    y_train_cv = y_train[train_index]\n",
    "                    y_val_cv = y_train[val_index]\n",
    "                    \n",
    "                    reset_keras()\n",
    "                    model = GRUModel(hyperparameters)\n",
    "                    hist, model = model.train(X_train_cv, y_train_cv, X_val_cv, y_val_cv)\n",
    "                \n",
    "                    v_hist.append(hist)\n",
    "                    v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "                    \n",
    "                metric_dev = np.mean(v_val_loss)\n",
    "                if metric_dev < bestMetricDev:\n",
    "                    print(\"\\t\\t\\tCambio the best\", bestMetricDev, \"por metric dev:\", metric_dev)\n",
    "                    bestMetricDev = metric_dev\n",
    "                    bestHyperparameters['dropout'] = k\n",
    "                    bestHyperparameters['layers'] = l\n",
    "                    bestHyperparameters['lr_scheduler'] = m\n",
    "\n",
    "    return bestHyperparameters, X_train_cv, X_val_cv, y_train_cv, y_val_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eca323",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4773e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seeds = [143, 45, 67]\n",
    "\n",
    "input_shape = 7\n",
    "# Select the first 24h - 24 time steps\n",
    "timeStep = 6\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "layer_list = [\n",
    "    [input_shape, 5, 1],\n",
    "    [input_shape, 3, 1],\n",
    "]\n",
    "dropout = [0.0, 0.15, 0.3]\n",
    "lr_scheduler = [0.5, 0.1, 0.01]\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"timeStep\": timeStep,\n",
    "    \"maskValue\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": 50,\n",
    "    \"kfold\": 5,\n",
    "    \"dropout\": dropout,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"layers\": layer_list,\n",
    "    \"verbose\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f27ecf",
   "metadata": {},
   "source": [
    "## Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2713e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLearning rate: 0.5 , dropout: 0.0 , layers: [7, 5, 1]\n",
      "\t\t\tCambio the best inf por metric dev: 0.6096979141235351\n",
      "\t\tLearning rate: 0.1 , dropout: 0.0 , layers: [7, 5, 1]\n",
      "\t\t\tCambio the best 0.6096979141235351 por metric dev: 0.5812863707542419\n",
      "\t\tLearning rate: 0.01 , dropout: 0.0 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.5 , dropout: 0.0 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.1 , dropout: 0.0 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.01 , dropout: 0.0 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.5 , dropout: 0.15 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.1 , dropout: 0.15 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.01 , dropout: 0.15 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.5 , dropout: 0.15 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.1 , dropout: 0.15 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.01 , dropout: 0.15 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.5 , dropout: 0.3 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.1 , dropout: 0.3 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.01 , dropout: 0.3 , layers: [7, 5, 1]\n",
      "\t\tLearning rate: 0.5 , dropout: 0.3 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.1 , dropout: 0.3 , layers: [7, 3, 1]\n",
      "\t\tLearning rate: 0.01 , dropout: 0.3 , layers: [7, 3, 1]\n",
      "\tlr_sch seleccionado: 0.1\n",
      "\tdropout seleccionado: 0.0\n",
      "\tlayers seleccionado: [7, 5, 1]\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\t\tLearning rate: 0.5 , dropout: 0.0 , layers: [7, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "metrics_data = []\n",
    "\n",
    "loss_train = []\n",
    "loss_dev = []\n",
    "v_models = []\n",
    "bestHyperparameters_bySplit = {}\n",
    "y_pred_by_split = []\n",
    "folders = [\"s1\", \"s2\", \"s3\"]\n",
    "\n",
    "for i in range(0,3):\n",
    "\n",
    "    X_train = np.load(\"../0_Data/splits/\" + folders[i] + \"/X_train_tensor.npy\")\n",
    "    y_train = np.load(\"../0_Data/splits/\" + folders[i] + \"/y_train_tensor.npy\")\n",
    "    y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "    \n",
    "    X_test = np.load(\"../0_Data/splits/\" + folders[i] + \"/X_test_tensor.npy\")\n",
    "    y_test = np.load(\"../0_Data/splits/\" + folders[i] + \"/y_test_tensor.npy\")\n",
    "    y_test = y_test.reshape(y_test.shape[0], y_test.shape[1], 1)\n",
    "\n",
    "    #GridSearch of hyperparameters and print them   \n",
    "    bestHyperparameters, X_train, X_val, y_train, y_val = myCVGrid(hyperparameters, seeds[i], X_train, y_train)\n",
    "    \n",
    "    bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "    print(\"\\tlr_sch seleccionado:\", lr_scheduler[bestHyperparameters[\"lr_scheduler\"]])\n",
    "    print(\"\\tdropout seleccionado:\", dropout[bestHyperparameters[\"dropout\"]])\n",
    "    print(\"\\tlayers seleccionado:\", layer_list[bestHyperparameters[\"layers\"]])\n",
    "    \n",
    "    \n",
    "    besthyperparameters = {\n",
    "        'timeStep': hyperparameters[\"timeStep\"],\n",
    "        'maskValue': hyperparameters[\"maskValue\"],\n",
    "        'batch_size': hyperparameters[\"batch_size\"],\n",
    "        'epochs': hyperparameters[\"epochs\"],\n",
    "        'monitor':  hyperparameters[\"monitor\"],\n",
    "        \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "        \"patience\": hyperparameters[\"patience\"],                    \n",
    "        \"dropout\": dropout[bestHyperparameters[\"dropout\"]],\n",
    "        \"layers\": layer_list[bestHyperparameters[\"layers\"]],\n",
    "        \"lr_scheduler\": lr_scheduler[bestHyperparameters[\"lr_scheduler\"]],                    \n",
    "        'kfold': hyperparameters[\"kfold\"],\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "#--- TRY ON TEST -----------------------------------------------------------------------#\n",
    "\n",
    "    #Reset keras\n",
    "    reset_keras()\n",
    "    model = GRUModel(besthyperparameters)\n",
    "    hist, model = model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    #Reset keras\n",
    "    reset_keras()\n",
    "    model = GRUModel(besthyperparameters)\n",
    "    hist, model = model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    y_pred = model.predict(x=X_test)\n",
    "    y_pred = np.reshape(y_pred, (y_pred.size,))\n",
    "    y_pred_by_split.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_metrics_over_time(n_time_steps, y_test_df, y_pred_df):\n",
    "    \"\"\"\n",
    "    Calculate metrics per time step.\n",
    "    Args:\n",
    "        - n_time_steps: Number of time steps.\n",
    "        - y_test_df : DataFrame containing the real values.\n",
    "        - y_pred_df : DataFrame containing the predicted values.\n",
    "    Returns:\n",
    "        - metrics_df: DataFrame containing the metrics for each time step.\n",
    "    \"\"\"\n",
    "    mask = (y_test_df == 666)\n",
    "    \n",
    "    # Lists to store the metrics\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "    tp_list = []\n",
    "    specificity_list = []\n",
    "    recall_list = []\n",
    "    roc_auc_list = []\n",
    "\n",
    "    # Calculate the metrics for each time step\n",
    "    for t in range(n_time_steps):\n",
    "        # Filter the valid values according to the mask\n",
    "        valid_indices = ~mask.iloc[:, t]\n",
    "        y_test_valid = y_test_df.iloc[:, t][valid_indices]\n",
    "        y_pred_valid = y_pred_df.iloc[:, t][valid_indices]\n",
    "\n",
    "        # Round the predictions\n",
    "        y_pred_rounded = np.round(y_pred_valid)\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_valid, y_pred_rounded, labels=[0, 1]).ravel()\n",
    "\n",
    "        # Calculate specifciity and recall\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "        # Calculate ROC-AUC\n",
    "        roc = roc_auc_score(y_test_valid, y_pred_valid) if len(np.unique(y_test_valid)) > 1 else np.nan\n",
    "\n",
    "        tn_list.append(tn)\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "        tp_list.append(tp)\n",
    "        specificity_list.append(specificity)\n",
    "        recall_list.append(recall)\n",
    "        roc_auc_list.append(roc)\n",
    "\n",
    "    # Dataframe to store the metrics per time step\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Time Step': range(1, n_time_steps+1),\n",
    "        'TN': tn_list,\n",
    "        'FP': fp_list,\n",
    "        'FN': fn_list,\n",
    "        'TP': tp_list,\n",
    "        'Specificity': specificity_list,\n",
    "        'Recall': recall_list,\n",
    "        'ROC AUC': roc_auc_list\n",
    "    })\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def plot_metrics_over_time(metrics_df, T):\n",
    "    \"\"\"\n",
    "    Plot metrics over time.\n",
    "    Args:\n",
    "        - metrics_df: DataFrame containing the metrics for each time step.\n",
    "        - T: Number of time steps.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.plot(metrics_df['Time Step'], metrics_df['Specificity'], label='Specificity', marker='o')\n",
    "    plt.plot(metrics_df['Time Step'], metrics_df['Recall'], label='Sensibility', marker='s')\n",
    "    plt.plot(metrics_df['Time Step'], metrics_df['ROC AUC'], label='ROC-AUC', marker='^')    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.xticks(ticks=range(1, T + 1))\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(folders)):\n",
    "    y_test = np.load(\"../0_Data/splits/\" + folders[i] + \"/y_test_tensor.npy\")\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "        \n",
    "    y_pred_df = pd.DataFrame(y_pred_by_split[i])\n",
    "\n",
    "    df_metrics = get_metrics_over_time(timeStep, y_test_df, y_pred_df)\n",
    "    plot_metrics_over_time(df_metrics, timeStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915758a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
